{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5de785-679a-4959-b533-1253acbbc64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "sys.path.append('/comfywr/diffusers/src/')\n",
    "\n",
    "def iter_subplots_axes(ncol, n_subplots, tile_size_col=5, tile_size_row=5, title=None, title_fontsize=14):\n",
    "    \"\"\" Creates subplots figure, and iterates over axes in left-right/top-bottom order \"\"\"\n",
    "    nrow = math.ceil(n_subplots / ncol)\n",
    "    fig, axes = plt.subplots(nrow, ncol)\n",
    "    if title is not None:\n",
    "        plt.suptitle(title, fontsize=title_fontsize)\n",
    "    fig.set_size_inches(ncol * tile_size_col, nrow * tile_size_row)\n",
    "    for i in range(n_subplots):\n",
    "        if nrow > 1 and ncol > 1:\n",
    "            ax = axes[i // ncol, i % ncol]\n",
    "        else:\n",
    "            if n_subplots > 1 or ncol > 1:\n",
    "                ax = axes[i]\n",
    "            else:\n",
    "                ax = axes\n",
    "        plt.sca(ax)\n",
    "        yield ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f27f98-1b49-4275-8521-5d8db6dde7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path('/comfywr/downloaded_models/checkpoints/DreamShaper_8_pruned.safetensors')\n",
    "stem = path.stem\n",
    "assert path.is_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973cb8ec-da79-488a-8575-c1d29125a28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!PYTHONPATH=/comfywr/diffusers/src python /comfywr/diffusers/scripts/convert_original_stable_diffusion_to_diffusers.py  \\\n",
    "    --checkpoint_path '/comfywr/downloaded_models/checkpoints/DreamShaper_8_pruned.safetensors' \\\n",
    "    --dump_path /comfywr/diffusers_tmp/chkp/ \\\n",
    "    --from_safetensors \\\n",
    "    --to_safetensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68e9858-c7c2-49f8-9397-c7c73c02a007",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from diffusers import DDPMPipeline\n",
    "from diffusers import DiffusionPipeline\n",
    "\n",
    "pipe = DiffusionPipeline.from_pretrained('/comfywr/diffusers_tmp/chkp/', use_safetensors=True).to('cuda')\n",
    "# ddpm = DDPMPipeline.from_pretrained('/comfywr/diffusers_tmp/chkp/', use_safetensors=True).to(\"cuda\")\n",
    "# ddpm = DDPMPipeline.from_pretrained(\"google/ddpm-cat-256\", use_safetensors=True).to(\"cuda\")\n",
    "image = pipe(prompt='a tree', num_inference_steps=25).images[0]\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3715621-9a8c-45db-8a05-4a7eb9425d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "from diffusers import AutoencoderKL, UNet2DConditionModel \n",
    "\n",
    "path = Path('/comfywr/diffusers_tmp/chkp/')\n",
    "vae = AutoencoderKL.from_pretrained(path, subfolder=\"vae\", use_safetensorfs=True)\n",
    "tokenizer = CLIPTokenizer.from_pretrained(path, subfolder=\"tokenizer\")\n",
    "text_encoder = CLIPTextModel.from_pretrained(path, subfolder=\"text_encoder\", use_safetensors=True)\n",
    "unet = UNet2DConditionModel.from_pretrained(path, subfolder=\"unet\", use_safetensors=True)\n",
    "torch_device = \"cuda\"\n",
    "vae = vae.to(torch_device)\n",
    "text_encoder = text_encoder.to(torch_device)\n",
    "unet = unet.to(torch_device)\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6382ebf-5ed9-4994-bfba-59211df844e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from diffusers import EulerAncestralDiscreteScheduler\n",
    "# scheduler = EulerAncestralDiscreteScheduler()\n",
    "from diffusers import UniPCMultistepScheduler\n",
    "scheduler = UniPCMultistepScheduler.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"scheduler\")\n",
    "\n",
    "prompt = [\"A purple tree on red grass\"]\n",
    "height = 512  # default height of Stable Diffusion\n",
    "width = 512  # default width of Stable Diffusion\n",
    "num_inference_steps = 25  # Number of denoising steps\n",
    "guidance_scale = 7.5  # Scale for classifier-free guidance\n",
    "generator = torch.Generator(device='cuda').manual_seed(0)\n",
    "batch_size = len(prompt)\n",
    "\n",
    "text_input = tokenizer(\n",
    "    prompt, padding=\"max_length\", max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\"\n",
    ")\n",
    "with torch.no_grad():\n",
    "    text_embeddings = text_encoder(text_input.input_ids.to(torch_device))[0]\n",
    "    print(text_embeddings.shape)\n",
    "\n",
    "max_length = text_input.input_ids.shape[-1]\n",
    "uncond_input = tokenizer([\"\"] * batch_size, padding=\"max_length\", max_length=max_length, return_tensors=\"pt\")\n",
    "uncond_embeddings = text_encoder(uncond_input.input_ids.to(torch_device))[0]\n",
    "\n",
    "text_embeddings = torch.cat([uncond_embeddings, text_embeddings])\n",
    "\n",
    "assert 2 ** (len(vae.config.block_out_channels) - 1) == 8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b0347e-5a8b-40bf-b823-338a1a32c82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert scheduler.init_noise_sigma == scheduler.sigmas[0]\n",
    "latents = torch.randn(\n",
    "    (batch_size, unet.config.in_channels, height // 8, width // 8),\n",
    "    generator=generator,\n",
    "    device=torch_device,\n",
    ")\n",
    "latents = latents * scheduler.init_noise_sigma\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "\n",
    "scheduler.set_timesteps(num_inference_steps)\n",
    "\n",
    "all_latents = [latents]\n",
    "\n",
    "for timestep_index, t in enumerate(tqdm(scheduler.timesteps)):\n",
    "    # expand the latents if we are doing classifier-free guidance to avoid doing two forward passes.\n",
    "    latent_model_input = torch.cat([latents] * 2)\n",
    "\n",
    "    # a = latent_model_input[0][0][0][0]\n",
    "    latent_model_input = scheduler.scale_model_input(latent_model_input, timestep=t)\n",
    "    # b = latent_model_input[0][0][0][0]\n",
    "    # assert abs(a / b - (scheduler.sigmas[timestep_index] ** 2 + 1) ** 0.5) < 0.00001, (a/b, scheduler.sigmas[i])\n",
    "\n",
    "    # predict the noise residual\n",
    "    with torch.no_grad():\n",
    "        noise_pred = unet(latent_model_input, t, encoder_hidden_states=text_embeddings).sample\n",
    "\n",
    "    # perform guidance\n",
    "    noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
    "    noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
    "\n",
    "    # compute the previous noisy sample x_t -> x_t-1\n",
    "    latents = scheduler.step(noise_pred, t, latents).prev_sample\n",
    "    all_latents.append(latents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67b0b07-b0f8-463c-a753-0e4a5b40c90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_imgs = []\n",
    "for latents in all_latents:\n",
    "    # the 0.18215 constant seems to be just used in VAE training -- TODO: is  it just experimental\n",
    "    latents = 1 / 0.18215 * latents\n",
    "    with torch.no_grad():\n",
    "        image = vae.decode(latents).sample\n",
    "        image = (image / 2 + 0.5).clamp(0, 1).squeeze()\n",
    "        image = (image.permute(1, 2, 0) * 255).to(torch.uint8).cpu().numpy()\n",
    "        all_imgs.append(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01649712-9f6c-4dbd-b69c-e5a41ad0bdb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for img, _ in zip(all_imgs, iter_subplots_axes(4, len(all_imgs))):\n",
    "    plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa09d87-48e2-4c0c-b286-53b3e85d6e0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9516707b-fb72-4ede-93ae-e5b8613b6967",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575b4f49-64d8-4fd3-8139-962f7df4adbf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
